{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6e34d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='http://localhost:11434' model='gemma3:1b' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=False headers=None model_kwargs=None\n"
     ]
    }
   ],
   "source": [
    "#Ollama : Ollama supports embedding models, Making it possible to build retrieval augmented generation (RAG) applications\n",
    "# that combine text prompts with existings doc's or other data.\n",
    "\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "#To use this lamma/gamma 3 to use this model here from linux machine. \n",
    "#ssh -L 11434:localhost:11434 gopinath.palanisami@10.227.244.87\n",
    "#This command forwards your Windows machine’s local port 11434 to the Linux machine’s localhost:11434.\n",
    "#Keep this terminal open while you want the tunnel active. Now, in Windows, you can access Ollama’s API as if it’s running locally:\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma3:1b\", base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "\n",
    "print(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "968080de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_url': FieldInfo(annotation=str, required=False, default='http://localhost:11434'), 'model': FieldInfo(annotation=str, required=False, default='llama2'), 'embed_instruction': FieldInfo(annotation=str, required=False, default='passage: '), 'query_instruction': FieldInfo(annotation=str, required=False, default='query: '), 'mirostat': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'mirostat_eta': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'mirostat_tau': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'num_ctx': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'num_gpu': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'num_thread': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'repeat_last_n': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'repeat_penalty': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'temperature': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'stop': FieldInfo(annotation=Union[List[str], NoneType], required=False, default=None), 'tfs_z': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'top_k': FieldInfo(annotation=Union[int, NoneType], required=False, default=None), 'top_p': FieldInfo(annotation=Union[float, NoneType], required=False, default=None), 'show_progress': FieldInfo(annotation=bool, required=False, default=False), 'headers': FieldInfo(annotation=Union[dict, NoneType], required=False, default=None), 'model_kwargs': FieldInfo(annotation=Union[dict, NoneType], required=False, default=None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palang3\\AppData\\Local\\Temp\\ipykernel_24080\\448527594.py:2: PydanticDeprecatedSince20: The `__fields__` attribute is deprecated, use `model_fields` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  print(OllamaEmbeddings.__fields__)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "print(OllamaEmbeddings.__fields__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6569885f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API HTTP code: 500, {\"error\":\"this model does not support embeddings\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m r1\u001b[38;5;241m=\u001b[39m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlpha is the first letter of Greek apl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBeta is the second letter of Greek apl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\palang3\\OneDrive - Dell Technologies\\Desktop\\AI_Friends\\GENAI\\LangChain\\venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:214\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed documents using an Ollama deployed embedding model.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    List of embeddings, one for each text.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m instruction_pairs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[1;32m--> 214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\palang3\\OneDrive - Dell Technologies\\Desktop\\AI_Friends\\GENAI\\LangChain\\venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32mc:\\Users\\palang3\\OneDrive - Dell Technologies\\Desktop\\AI_Friends\\GENAI\\LangChain\\venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[1;32mc:\\Users\\palang3\\OneDrive - Dell Technologies\\Desktop\\AI_Friends\\GENAI\\LangChain\\venv\\lib\\site-packages\\langchain_community\\embeddings\\ollama.py:176\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API HTTP code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;241m%\u001b[39m (res\u001b[38;5;241m.\u001b[39mstatus_code, res\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    179\u001b[0m     )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m     t \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by inference API HTTP code: 500, {\"error\":\"this model does not support embeddings\"}"
     ]
    }
   ],
   "source": [
    "r1=embeddings.embed_documents(\n",
    "    [\n",
    "        \"Alpha is the first letter of Greek apl\", \n",
    "        \"Beta is the second letter of Greek apl\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# which will convert text into vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query from embeddings \n",
    "\n",
    "embeddings.embed_query(\"what is the second letter of Greek alph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da963a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7039f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
