{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63424ddf",
   "metadata": {},
   "source": [
    "Building Chatbots\n",
    "\n",
    "Design and build / Implement an LLM powered chatbots. This chatbot will be able to have a conversation and remember prevoius interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fa9ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build chatbot with LLMs\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#Loading environment variables.\n",
    "load_dotenv() \n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdeeea74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000021F53B7B640>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000021F53B7B550>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"llama-3.1-8b-instant\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf41a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Gopi. It's great to hear that you're a chief AI Engineer. That's a fascinating role, and I'm sure you must be working on exciting projects. What specific areas of AI are you currently focusing on, or are there any particular projects that you're passionate about?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 49, 'total_tokens': 112, 'completion_time': 0.120202509, 'prompt_time': 0.00349131, 'queue_time': 0.04439699, 'total_time': 0.123693819}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--79475553-0e61-48c1-a27b-5b6e2eb89d7e-0', usage_metadata={'input_tokens': 49, 'output_tokens': 63, 'total_tokens': 112})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi, My name is Gopi and I am a chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44156d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Gopi, and you are a Chief AI Engineer.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 132, 'total_tokens': 147, 'completion_time': 0.019715294, 'prompt_time': 0.007742432, 'queue_time': 0.044372748, 'total_time': 0.027457726}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_e32974efee', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3d71adc6-ccdd-4020-ad76-5b2d444e467e-0', usage_metadata={'input_tokens': 132, 'output_tokens': 15, 'total_tokens': 147})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import AIMessage\n",
    "#This is going to give me answer about what I gave (As Input and give me output based on it)\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi, My name is Gopi and I am a chief AI Engineer\"),\n",
    "        AIMessage(content=\"Nice to meet you, Gopi. It's great to hear that you're a chief AI Engineer. That's a fascinating role, and I'm sure you must be working on exciting projects. What specific areas of AI are you currently focusing on, or are there any particular projects that you're passionate about?\"),\n",
    "        HumanMessage(content=\"Hi What's my name and what do i do ?\")\n",
    "\n",
    "                  \n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1adadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Different sessions will there how it will be remerbered in LLM models. \n",
    "\n",
    "# Message History \n",
    "#Why Message History Matters\n",
    "\n",
    "#Large Language Models (LLMs) like GPT are stateless — they don’t remember past interactions unless you explicitly provide that information again. \n",
    "# To simulate a conversation or maintain continuity, you send the previous messages back with each request.\n",
    "\n",
    "#This is how the model:\n",
    "\n",
    "#Maintains the conversation flow.\n",
    "\n",
    "#Remembers previous questions/answers.\n",
    "\n",
    "#Understands follow-up queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3abe2",
   "metadata": {},
   "source": [
    "| Class/Concept                | Description                                                                                                                   |\n",
    "| ---------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `ChatMessageHistory`         | A simple container to store a sequence of chat messages between user and AI.                                                  |\n",
    "| `BaseChatMessageHistory`     | An **abstract base class** that defines the interface all message history classes must follow.                                |\n",
    "| `RunnableWithMessageHistory` | A wrapper around LangChain **Runnables** (chains, tools, agents, etc.) that automatically handles message history and memory. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9493a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "#This is to make sure one session is compeletely different from other session\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_messgae_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9543f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a config\n",
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a4fc42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Gopi. Welcome to our conversation. As an engineer, I'm sure you have a strong foundation in problem-solving and technical skills. What kind of engineering do you specialize in, and what are some of the projects you've worked on?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 47, 'total_tokens': 101, 'completion_time': 0.076798886, 'prompt_time': 0.002182097, 'queue_time': 0.044801149, 'total_time': 0.078980983}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_33e8adf159', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f7a82fb4-20ef-4754-927c-0bee062256f7-0', usage_metadata={'input_tokens': 47, 'output_tokens': 54, 'total_tokens': 101})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Am trying intract with AI based on Session ID.\n",
    "\n",
    "with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My name is Gopi and am a Engineer.\")],\n",
    "    #Based on Seesion ID will interact with AI \n",
    "    config=config\n",
    "\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e041e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Gopi.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 115, 'total_tokens': 122, 'completion_time': 0.005166304, 'prompt_time': 0.006265769, 'queue_time': 0.044851721, 'total_time': 0.011432073}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--d3189076-c55a-4e5c-b343-d0219c09904c-0', usage_metadata={'input_tokens': 115, 'output_tokens': 7, 'total_tokens': 122})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human Message , Same Session ID / Same config >> Its going to remember my name give me correct name.\n",
    "\n",
    "with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"what's my name ?\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7bc6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your name. I'm a large language model, I don't have the ability to know or store personal information about individuals. Each time you interact with me, it's a new conversation and I don't retain any information from previous conversations. If you'd like to share your name, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the session ID ( which is config )\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response=with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name ?\")],\n",
    "    config=config1\n",
    "\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c26f59da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello John, it's nice to meet you. Since we're starting a new conversation, I'll just acknowledge that's what you've shared with me. What's on your mind today, John? Would you like to talk about something in particular or just see where the conversation goes?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"My name is John?\")],\n",
    "    config=config1\n",
    "\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79eed3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've already established that your name is John in our previous conversation. Is there something you'd like to clarify or change about that?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re=with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"what is my name ?\")],\n",
    "    config=config1\n",
    "\n",
    ")\n",
    "re.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985608ef",
   "metadata": {},
   "source": [
    "#Prompt Templates\n",
    "\n",
    "Prompt templates help to turn raw user information into format that the LLM can work with. \n",
    "In case the user user input is just a message, Which we are passing to LLM.\n",
    "System message - Some custom instructions (but still taking messages as input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ff1136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4601a080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Gopi. I'm here to help with any questions or topics you'd like to discuss. How's your day going so far?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 57, 'total_tokens': 90, 'completion_time': 0.036403515, 'prompt_time': 0.003023512, 'queue_time': 0.044799298, 'total_time': 0.039427027}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ab04adca7d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b1008220-093c-45cd-90ca-4117f55abfd6-0', usage_metadata={'input_tokens': 57, 'output_tokens': 33, 'total_tokens': 90})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Gopi\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e942150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to use ChatMessage History her in prompt\n",
    "\n",
    "with_messgae_history=RunnableWithMessageHistory(chain,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81580747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, DADA. Is there something I can help you with today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response=with_messgae_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is DADA\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add more complexity (Add Language)\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae9f1e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'வணக்கம்! நான் உங்களுக்கு உதவி செய்ய இங்கு இருக்கிறேன். நீங்கள் DADA என்று பெயர் பெற்றிருக்கிறீர்களா?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Key value pair, Message , language\n",
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is DADA\")], \"language\":\"Tamil\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437fb24",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now wrap this more complicated chain in a message history class.\n",
    "\n",
    "This time because there are multiple keys in the input, we need to specify the correct key used to\n",
    "\n",
    "save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d68f5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_messgae_history=RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86ae2247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'வணக்கம் தங்கை தடா. நலமா உங்க வாழ்க்கையில.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= {\"configurable\": {\"session_id\": \"chat4\"}}\n",
    "re = with_messgae_history.invoke(\n",
    "    {'messages': [HumanMessage(content=\"Hi,I am dada\")], \"language\":\"Tamil\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "re.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b28620f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'உன் பெயர் தடா என்று நீ கூறினாலும், நீ தடா என்று கேட்டாலும், நான் தடா என்றுதான் பதில் கூறினேன்.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re=with_messgae_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Tamil\"},\n",
    "        config=config,\n",
    ")\n",
    "re.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de6d54",
   "metadata": {},
   "source": [
    "Managing the chat conversion History using LANGCHAIN:\n",
    "\n",
    "Now we are going to focus on understanding how do we manage the conversation history.\n",
    "See, one important concept to understand when building chatbot is how to manage conversation history.If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM.\n",
    "\n",
    "Therefore, it is important to add a step that limits the size of the messages that you are passing it.\n",
    "\n",
    "Trim_messages :: It is a helper to reduce How many messages we are sending to the model. \n",
    "The trimmer allows us to specify how many tokens we want to keep, along with other parameters \n",
    "like if we wanted to always keep the systems messages and whether to allow partial messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "463429f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='your are a good assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like Ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2+2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    #max_tokens=70,\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"your are a good assistant\"),\n",
    "    HumanMessage(content=\"Hi ! I'm bob\"),\n",
    "    AIMessage(content=\"HI\"),\n",
    "    HumanMessage(content=\"I like Ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"what's 2+2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem\"),\n",
    "    HumanMessage(content=\"Having fun?\"),\n",
    "    AIMessage(content=\"Yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now u can change the max_token=25 (Instead of 70 , So you limit the size of chat / History.)\n",
    "'''[SystemMessage(content='your are a good assistant', additional_kwargs={}, response_metadata={}),\n",
    " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
    " AIMessage(content='no problem', additional_kwargs={}, response_metadata={}),\n",
    " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
    " AIMessage(content='Yes!', additional_kwargs={}, response_metadata={})]'''\n",
    "\n",
    "### Won't have full converstions / top messages have be trimmed off. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fec09c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Unfortunately, I don't know what you like, as we just started our conversation. Would you like to share what you like?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In as chain how we pass the trimmer \n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter('messages')|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what I like ?\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content\n",
    "## Why I got don't have information is because I giving max_token as 25 in trimmer that's why. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7243a55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You gave me the math problem \"2+2\"'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"what math problem I gave  ??\")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47f5b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's Wrap with message history\n",
    "\n",
    "with_messgae_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eacb89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
